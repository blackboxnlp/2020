
[Main page](index.md)

Summary of the program (Time indications: Punta Cana)
----------------------

| Session                           | Time          |
|---------------------------------------------------|
| Opening remarks                   | 4:00 - 4:15   |
| Keynote speaker 1 -- Anna Rogers  | 4:15 - 5:00   |
| Break                                             |
| Oral presentations 1              | 5:15 - 6:00   |
| Break                                             |
| Demo presentation                 | 6:15 - 6:30   |
| Poster session B                  | 6:30 - 8:00   |
| Break                                             |
| Keynote speaker 1 -- Anna Rogers  | 8:15 - 9:00   |
|---------------------------------------------------|
| Break                                             |
|---------------------------------------------------|
| Keynote speaker 2 -- Roger Levy   | 11:00 - 11:45 |
| Break                                             |
| Oral presentations 2              | 12:00 - 13:00 |
| Break                                             |
| Keynote speaker 3 -- Idan Blank   | 13:15 - 14:00 |
| Awards and closing remarks        | 14:00 - 14:20 |
| Demo presentation - repetition    | 14:25 - 14:40 |
| Break                                             |
| Poster session C                  | 14:30 - 16:00 |
|---------------------------------------------------|
| Break                                             |
|---------------------------------------------------|
| Keynote speaker 2 -- Roger Levy   | 19:00 - 19:45 |
| Break                                             |
| Poster session A                  | 20:00 - 21:30 |            
| Oral session 3                    | 21:30 - 22:45 |
| Break                                             |
| Keynote speaker 3 -- Idan Blank   | 23:00 - 23:45 |


Oral presentation session 1 (Time indications: Punta Cana)
---------------------------
- 5:15 - 5:27 _The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?_. Jasmijn Bastings and Katja Filippova .
- 5:27 - 5:39 _BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance_. R. Thomas Mccoy, Junghyun Min and Tal Linzen.
- 5:39 - 5:51 _Evaluating Attribution Methods using White-Box LSTMs_. Yiding Hao.
- 5:51 - 6:00 _Live Q&A_ with all paper authors.

Oral presentation session 2 (Time indications: Punta Cana)
---------------------------

- 12:00 - 12:12 _What Happens To BERT Embeddings During Fine-tuning?_. Amil Merchant, Elahe Rahimtoroghi, Ellie Pavlick and Ian Tenney.
- 12:12 - 12:24 _The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?_. Jasmijn Bastings and Katja Filippova  (repetition).
- 12:24 - 12:36 _Dissecting Lottery Ticket Transformers: Structural and Behavioral Study of Sparse Neural Machine Translation_. Dissecting Lottery Ticket Transformers: Structural and Behavioral Study of Sparse Neural Machine Translation.
- 12:36 - 12:48 _The EOS Decision and Length Extrapolation_. Benjamin Newman, John Hewitt, Percy Liang and Christopher D. Manning.
- 12:48 - 13:00 _Live Q&A_ With all paper authors.

Oral presentation session 3 (Time indications: Punta Cana)
---------------------------
- 21:30 - 21:42 _Evaluating Attribution Methods using White-Box LSTMs_. Yiding Hao (repetition).
- 21:42 - 21:54 _The EOS Decision and Length Extrapolation_. Benjamin Newman, John Hewitt, Percy Liang and Christopher D. Manning (repetition).
- 21:54 - 22:06 _BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance_. R. Thomas Mccoy, Junghyun Min and Tal Linzen (repetition).
- 22:06 - 22:18 _Dissecting Lottery Ticket Transformers: Structural and Behavioral Study of Sparse Neural Machine Translation_. Dissecting Lottery Ticket Transformers: Structural and Behavioral Study of Sparse Neural Machine Translation (repetition).
- 22:18 - 22:30  _What Happens To BERT Embeddings During Fine-tuning?_. Amil Merchant, Elahe Rahimtoroghi, Ellie Pavlick and Ian Tenney (repetition).
- 22:30 - 22:45 _Live Q&A_ With all paper authors.

Poster session 1 (block B)
----------------
**Archival papers**
- _The Explanation Game: Towards Prediction Explainability through Sparse Communication_  Marcos Treviso and André F. T. Martins (room A).
- _Defining Explanation in an AI Context_  Tejaswani Verma, Christoph Lingenfelder and Dietrich Klakow (room A).
- _Controlling the Imprint of Passivization and Negation in Contextualized Representations_  Hande Celikkanat, Sami Virpioja, Jörg Tiedemann and Marianna Apidianaki (room A).
- _How does BERT capture semantics? A closer look at polysemous words_  David Yenicelik, Florian Schmidt and Yannic Kilcher (room A).
- _Leveraging Extracted Model Adversaries for Improved Black Box Attacks_. Naveen Jafer Nizar and Ari Kobren (room B).
- _Attacking Semantic Similarity: Generating Second-Order NLP Adversarial Examples_  John Morris (room B).
- _BERTnesia: Investigating the capture and forgetting of knowledge in BERT_  Jaspreet Singh, Jonas Wallat and Avishek Anand (room B).
- _Linguistically-Informed Transformations (LIT): A Method for Automatically Generating Contrast Sets_  Chuanrong Li, Lin Shengshuo, Zeyu Liu, Xinyi Wu, Xuhui Zhou and Shane Steinert-Threlkeld (room B).
- _Latent Tree Learning with Ordered Neurons: What Parses Does It Produce?_  Yian Zhang (room B).
- _Emergent Language Generalization and Acquisition Speed are not tied to Compositionality_. Eugene Kharitonov and Marco Baroni (room C).
- _Neural Natural Language Inference Models Partially Embed Theories of Lexical Entailment and Negation_  Atticus Geiger, Kyle Richardson and Christopher Potts (room C).
- _It’s not Greek to mBERT: Inducing Word-Level Translations from Multilingual BERT_. Hila Gonen, Shauli Ravfogel, Yanai Elazar and Yoav Goldberg (room C).
- _On the Interplay Between Fine-tuning and Sentence-Level Probing for Linguistic Knowledge in Pre-Trained Transformers_. Marius Mosbach, Anna Khokhlova, Michael A. Hedderich and Dietrich Klakow (room C).
- _Unsupervised Evaluation for Question Answering with Transformers_. Lukas Muttenthaler, Isabelle Augenstein and Johannes Bjerva (room D).
- _Do Language Embeddings capture Scales?_  Xikun Zhang, Deepak Ramachandran, Ian Tenney, Yanai Elazar and Dan Roth (room D).
- _Searching for a Search Method: Benchmarking Search Algorithms for Generating NLP Adversarial Examples_  Jin Yong Yoo, John Morris, Eli Lifland and Yanjun Qi (room D).

**Extended abstracts**
- _How Much Does RoBERTa Know About Quantifiers? An Assessment via Natural Language Inference_. Authors:  Cedegao Zhang (room A).
- _Inductive Biases of Seq2seq Learners under Minimal Stimuli_. Eugene Kharitonov and Rahma Chaabouni (room A).
- _Interpreting Neural Networks with Topic Models: Evidence from "Predicting In-game Actions From Interviews of NBA Players"_. Amir Feder, Nadav Oved and Roi Reichart (room A).
- _The Linear Geometry of Contextualized Word Representations_. Evan Hernandez and Jacob Andreas (room B).
- _Mighty Morpho-Probing Models_. Naomi Tachikawa Shapiro, Amandalynne Paullada and Shane Steinert-Threlkeld (room C).
- _CausaLM: Causal Model Explanation Through Counterfactual Language Models_. Amir Feder, Nadav Oved, Uri Shalit and Roi Reichart (room C).
- _Analyzing saliency in neural content scoring models for science explanations_. Brian Riordan, Sarah Bichler, Allison Bradford and Marcia Linn (room C). 
- _Analyzing Neural Machine Translation Trained Using Targeted Part-Of-Speech_. Subhadarshi Panda (room C).

**Findings papers**
- _Improving Text Understanding via Deep Syntax-Semantics Communication_. Hao Fei, Yafeng Ren and Donghong Ji (room A).
- _On the Language Neutrality of Pre-trained Multilingual Representations_. Jindřich Libovický, Rudolf Rosa and Alexander Fraser (room A).
- _Corpora Evaluation and System Bias Detection in Multi-document Summarization_. Alvin Dey, Tanya Chowdhury, Yash Kumar and Tanmoy Chakraborty (room A).
- _Interpretable Entity Representations through Large-Scale Typing_. Yasumasa Onoe and Greg Durrett (room B).
- _Be Different to Be Better! A Benchmark to Leverage the Complementarity of Language and Vision_. Sandro Pezzelle, Claudio Greco, Greta Gandolfi, Eleonora Gualdoni and Raffaella Bernardi (room B).
- _NLP Service APIs and Models for Efficient Registration of New Clients_. Sahil Shah, Vihari Piratla, Soumen Chakrabarti and Sunita Sarawagi (room B).
- _Rethinking Self-Attention: Towards Interpretability in Neural Parsing_. Khalil Mrini, Franck Dernoncourt, Quan Hung Tran, Trung Bui, Walter Chang and Ndapa Nakashole (room B).
- _Reducing Sentiment Bias in Language Models via Counterfactual Evaluation_. Weijie Feng, Binbin Liu, Dongpeng Xu, Qilong Zheng and Yun Xu (room C).
- _Undersensitivity in Neural Reading Comprehension_. Johannes Welbl, Pasquale Minervini, Max Bartolo, Pontus Stenetorp and Sebastian Riedel (room D).
- _Assessing Robustness of Text Classification through Maximal Safe Radius Computation_. Emanuele La Malfa, Min Wu, Luca Laurenti, Benjie Wang, Anthony Hartshorn and Marta Kwiatkowska (room D). 
- _Universal Dependencies According to BERT: Both More Specific andMore General_. Tomasz Limisiewicz, David Mareček and Rudolf Rosa (room D).
- _LSTMS Compose---and Learn---Bottom-Up_. Naomi Saphra and Adam Lopez (room D).
- _What's so special about BERT's layers? A closer look at the NLP pipeline in monolingual and multilingual models_. Wietse de Vries, Andreas van Cranenburgh and Malvina Nissim (room D).

Poster session 2 (block C)
----------------
**Archival papers**
- _BERTnesia: Investigating the capture and forgetting of knowledge in BERT_  Jaspreet Singh, Jonas Wallat and Avishek Anand (room J).
- _Probing for Multilingual Numerical Understanding in Transformer-Based Language Models_  Devin Johnson, Denise Mak, Andrew Barker and Lexi Loessberg-Zahl (room J).
- _Examining the rhetorical capacities of neural language models_  Zining Zhu, Chuer Pan, Mohamed Abdalla and Frank Rudzicz (room J).
- _On the Interplay Between Fine-tuning and Sentence-Level Probing for Linguistic Knowledge in Pre-Trained Transformers_  Marius Mosbach, Anna Khokhlova, Michael A. Hedderich and Dietrich Klakow (room J).
- _Emergent Language Generalization and Acquisition Speed are not tied to Compositionality_  Eugene Kharitonov and Marco Baroni (room K).
- _Unsupervised Distillation of Syntactic Information from Contextualized Word Representations_  Shauli Ravfogel, Yanai Elazar, Jacob Goldberger and Yoav Goldberg (room K).
- _It’s not Greek to mBERT: Inducing Word-Level Translations from Multilingual BERT_  Hila Gonen, Shauli Ravfogel, Yanai Elazar and Yoav Goldberg (room K).
- _Investigating Novel Verb Learning in BERT: Selectional Preference Classes and Alternation-Based Syntactic Generalization_  Tristan Thrush, Ethan Wilcox and Roger Levy (room K).
- _Discovering the Compositional Structure of Vector Representations with Role Learning Networks_  Paul Soulos, R. Thomas Mccoy, Tal Linzen and Paul Smolensky (room L).
- _Unsupervised Evaluation for Question Answering with Transformers_  Lukas Muttenthaler, Isabelle Augenstein and Johannes Bjerva (room L).
- _This is a BERT. Now there are several of them. Can they generalize to novel words?_  Coleman Haley (room L).
- _Leveraging Extracted Model Adversaries for Improved Black Box Attacks_  Naveen Jafer Nizar and Ari Kobren (room L).
- _Latent Tree Learning with Ordered Neurons: What Parses Does It Produce?_  Yian Zhang (room L).
- _BERTering RAMS: What and How Much does BERT Already Know About Event Arguments? - A Study on the RAMS Dataset_  Varun Gangal and Eduard Hovy (room M).
- _The Explanation Game: Towards Prediction Explainability through Sparse Communication_  Marcos Treviso and André F. T. Martins (room M).
- _Controlling the Imprint of Passivization and Negation in Contextualized Representations_  Hande Celikkanat, Sami Virpioja, Jörg Tiedemann and Marianna Apidianaki (room M).
- _How does BERT capture semantics? A closer look at polysemous words_  David Yenicelik, Florian Schmidt and Yannic Kilcher (room M).
- _Exploring Neural Entity Representations for Semantic Information_  Andrew Runge and Eduard Hovy (room M).
- _Structured Self-Attention Weights Encodes Semantics in Sentiment Analysis_  Zhengxuan Wu, Thanh-Son Nguyen and Desmond Ong (room M).
- _Defining Explanation in an AI Context_  Tejaswani Verma, Christoph Lingenfelder and Dietrich Klakow (room M).

**Extended abstracts**
- _Analyzing Neural Machine Translation Trained Using Targeted Part-Of-Speech_. Subhadarshi Panda (room K).
- _BERT's Adaptability to Small Data_. Héctor Vázquez Martínez and Annika Heuser (room L).

**Findings papers**
- _Optimizing Word Segmentation for Downstream Task_. Tatsuya Hiraoka, Sho Takase, Kei Uchiumi, Atsushi Keyaki and Naoaki Okazaki (room J).
- _Universal Dependencies According to BERT: Both More Specific andMore General_. Tomasz Limisiewicz, David Mareček and Rudolf Rosa (room J).
- _Natural Language Rationales with Full-Stack Visual Reasoning: From Pixels to Semantic Frames to Commonsense Graphs_. Ana Marasović, Chandra Bhagavatula, Jae sung Park, Ronan Le Bras, Noah A. Smith and Yejin Choi (room J).
- _Evaluating Factuality in Generation with Dependency-level Entailment_. Tanya Goyal and Greg Durrett (room J).
- _What's so special about BERT's layers? A closer look at the NLP pipeline in monolingual and multilingual models_. Wietse de Vries, Andreas van Cranenburgh and Malvina Nissim (room J).
- _On the Sub-Layer Functionalities of Transformer Decoder_ Yilin Yang, Longyue Wang, Shuming Shi, Prasad Tadepalli, Stefan Lee and Zhaopeng Tu (room J).
- _Leakage-Adjusted Simulatability: Can Models Generate Non-Trivial Explanations of Their Behavior in Natural Language?_. Peter Hase, Shiyue Zhang, Harry Xie and Mohit Bansal (room K).
- _Interpretable Entity Representations through Large-Scale Typing_. Yasumasa Onoe and Greg Durrett (room K).
- _Towards Controllable Biases in Language Generation_. Emily Sheng, Kai-Wei Chang, Prem Natarajan and Nanyun Peng (room K).
- _Be Different to Be Better! A Benchmark to Leverage the Complementarity of Language and Vision_. Sandro Pezzelle, Claudio Greco, Greta Gandolfi, Eleonora Gualdoni and Raffaella Bernardi (room K).
- _How Can Self-Attention Networks Recognize Dyck-n Languages?_. Javid Ebrahimi, Dhruv Gelda and Wei Zhang (room L).
- _NLP Service APIs and Models for Efficient Registration of New Clients_. Sahil Shah, Vihari Piratla, Soumen Chakrabarti and Sunita Sarawagi (room L). 
- _Why and when should you pool? Analyzing Pooling in Recurrent Architectures_. Pratyush Maini, Keshav Kolluru, Danish Pruthi and Mausam (room L). 
- _Exploring BERT's sensitivity to lexical cues using tests from semantic priming_. Kanishka Misra, Allyson Ettinger and Julia Rayz (room M). 
- _LSTMS Compose---and Learn---Bottom-Up_. Naomi Saphra and Adam Lopez (room M). 

Poster session 3 (block A)
----------------
**Archival papers**
- _BERTering RAMS: What and How Much does BERT Already Know About Event Arguments? - A Study on the RAMS Dataset_  Varun Gangal and Eduard Hovy (room A).
- _Exploring Neural Entity Representations for Semantic Information_  Andrew Runge and Eduard Hovy (room A).
- _Structured Self-Attention Weights Encodes Semantics in Sentiment Analysis_  Zhengxuan Wu, Thanh-Son Nguyen and Desmond Ong (room A).
- _This is a BERT. Now there are several of them. Can they generalize to novel words?_  Coleman Haley (room B).
- _Attacking Semantic Similarity: Generating Second-Order NLP Adversarial Examples_  John Morris (room B).
- _Discovering the Compositional Structure of Vector Representations with Role Learning Networks_  Paul Soulos, R. Thomas Mccoy, Tal Linzen and Paul Smolensky (room B).
- _Examining the rhetorical capacities of neural language models_  Zining Zhu, Chuer Pan, Mohamed Abdalla and Frank Rudzicz (room B).
- _Linguistically-Informed Transformations (LIT): A Method for Automatically Generating Contrast Sets_  Chuanrong Li, Lin Shengshuo, Zeyu Liu, Xinyi Wu, Xuhui Zhou and Shane Steinert-Threlkeld (room B).
- _Unsupervised Distillation of Syntactic Information from Contextualized Word Representations_  Shauli Ravfogel, Yanai Elazar, Jacob Goldberger and Yoav Goldberg (room C).
- _Investigating Novel Verb Learning in BERT: Selectional Preference Classes and Alternation-Based Syntactic Generalization_  Tristan Thrush, Ethan Wilcox and Roger Levy (room C).
- _Neural Natural Language Inference Models Partially Embed Theories of Lexical Entailment and Negation_  Atticus Geiger, Kyle Richardson and Christopher Potts (room C).
- _Probing for Multilingual Numerical Understanding in Transformer-Based Language Models_  Devin Johnson, Denise Mak, Andrew Barker and Lexi Loessberg-Zahl (room D). 
- _Do Language Embeddings capture Scales?_  Xikun Zhang, Deepak Ramachandran, Ian Tenney, Yanai Elazar and Dan Roth (room D).
- _Searching for a Search Method: Benchmarking Search Algorithms for Generating NLP Adversarial Examples_  Jin Yong Yoo, John Morris, Eli Lifland and Yanjun Qi (room D).

**Extended abstracts**
- _Inductive Biases of Seq2seq Learners under Minimal Stimuli_. Eugene Kharitonov and Rahma Chaabouni (room A).
- _How Much Does RoBERTa Know About Quantifiers? An Assessment via Natural Language Inference_. Authors:  Cedegao Zhang (room A).
- _Interpreting Neural Networks with Topic Models: Evidence from "Predicting In-game Actions From Interviews of NBA Players"_. Amir Feder, Nadav Oved and Roi Reichart (room A).
- _BERT's Adaptability to Small Data_. Héctor Vázquez Martínez and Annika Heuser (room B).
- _The Linear Geometry of Contextualized Word Representations_. Evan Hernandez and Jacob Andreas (room B).
- _CausaLM: Causal Model Explanation Through Counterfactual Language Models_. Amir Feder, Nadav Oved, Uri Shalit and Roi Reichart (room C).
- _Analyzing saliency in neural content scoring models for science explanations_. Brian Riordan, Sarah Bichler, Allison Bradford and Marcia Linn (room C).
- _Mighty Morpho-Probing Models_. Naomi Tachikawa Shapiro, Amandalynne Paullada and Shane Steinert-Threlkeld (room C).

**Findings papers**
- _Improving Text Understanding via Deep Syntax-Semantics Communication_. Hao Fei, Yafeng Ren and Donghong Ji (room A).
- _Exploring BERT's sensitivity to lexical cues using tests from semantic priming_. Kanishka Misra, Allyson Ettinger and Julia Rayz (room A). 
- _Corpora Evaluation and System Bias Detection in Multi-document Summarization_. Alvin Dey, Tanya Chowdhury, Yash Kumar and Tanmoy Chakraborty (room A).
- _On the Language Neutrality of Pre-trained Multilingual Representations_. Jindřich Libovický, Rudolf Rosa and Alexander Fraser (room A).
- _How Can Self-Attention Networks Recognize Dyck-n Languages?_. Javid Ebrahimi, Dhruv Gelda and Wei Zhang (room B).3210
- _Why and when should you pool? Analyzing Pooling in Recurrent Architectures_. Pratyush Maini, Keshav Kolluru, Danish Pruthi and Mausam (room B).
- _Rethinking Self-Attention: Towards Interpretability in Neural Parsing_. Khalil Mrini, Franck Dernoncourt, Quan Hung Tran, Trung Bui, Walter Chang and Ndapa Nakashole (room B).
- _Reducing Sentiment Bias in Language Models via Counterfactual Evaluation_. Weijie Feng, Binbin Liu, Dongpeng Xu, Qilong Zheng and Yun Xu (room C).
- _Towards Controllable Biases in Language Generation_. Emily Sheng, Kai-Wei Chang, Prem Natarajan and Nanyun Peng (room C).
- _Leakage-Adjusted Simulatability: Can Models Generate Non-Trivial Explanations of Their Behavior in Natural Language?_. Peter Hase, Shiyue Zhang, Harry Xie and Mohit Bansal (room C).
- _Event-Related Bias Removal for Real-time Disaster Events_. Salvador Medina Maza, Evangelia Spiliopoulou, Eduard Hovy and Alexander Hauptmann (room C). 
- _Undersensitivity in Neural Reading Comprehension_. Johannes Welbl, Pasquale Minervini, Max Bartolo, Pontus Stenetorp and Sebastian Riedel (room D).
- _Natural Language Rationales with Full-Stack Visual Reasoning: From Pixels to Semantic Frames to Commonsense Graphs_. Ana Marasović, Chandra Bhagavatula, Jae sung Park, Ronan Le Bras, Noah A. Smith and Yejin Choi (room D). 
- _Optimizing Word Segmentation for Downstream Task_. Tatsuya Hiraoka, Sho Takase, Kei Uchiumi, Atsushi Keyaki and Naoaki Okazaki (room D).
- _Assessing Robustness of Text Classification through Maximal Safe Radius Computation_. Emanuele La Malfa, Min Wu, Luca Laurenti, Benjie Wang, Anthony Hartshorn and Marta Kwiatkowska (room D). 
- _Evaluating Factuality in Generation with Dependency-level Entailment_. Tanya Goyal and Greg Durrett (room D). 
- _Weakly- and Semi-supervised Evidence Extraction_. Danish Pruthi, Bhuwan Dhingra, Graham Neubig and Zachary C. Lipton (room D).
- _On the Sub-Layer Functionalities of Transformer Decoder_ Yilin Yang, Longyue Wang, Shuming Shi, Prasad Tadepalli, Stefan Lee and Zhaopeng Tu (room D).


