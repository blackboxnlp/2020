
[Main page](index.md)

All plenary sessions will be held in zoom, where we will livestream the keynote and oral presentations, that are followed by a live Q&A. Questions can be asked in rocket chat during the presentations. Poster sessions will be held in gather town. **The links to the zoom session, gather.town space and the rocketchat channel can be found on the [EMNLP page of the workshop](https://virtual.2020.emnlp.org/workshop_WS-25.html)** (only accessible with conference registration).

Summary of the program (Time indications: Punta Cana)
----------------------

| Session                           | Time          | Location              |
|---------------------------------------------------|                       |
| Opening remarks                   | 4:00 - 4:15   | Zoom                  |
| Keynote speaker 1 -- Anna Rogers  | 4:15 - 5:00   | Zoom                  |
| Break                                             |                       |
| Oral presentations 1              | 5:15 - 6:00   | Zoom                  |
| Break                                             |                       |
| Demo presentation                 | 6:15 - 6:30   | Zoom                  |
| Poster session B                  | 6:30 - 8:00   | gather.town room K-N  |
| Break                                             |                       |
| Keynote speaker 1 -- Anna Rogers  | 8:15 - 9:00   | Zoom                  |
|---------------------------------------------------|                       |
| Break                                             |                       |
|---------------------------------------------------|                       |
| Keynote speaker 2 -- Roger Levy   | 11:00 - 11:45 | Zoom                  |
| Break                                             |                       |
| Oral presentations 2              | 12:00 - 13:00 | Zoom                  |
| Break                                             |                       |
| Keynote speaker 3 -- Idan Blank   | 13:15 - 14:00 | Zoom                  |
| Awards and closing remarks        | 14:00 - 14:20 | Zoom                  |
| Demo presentation - repetition    | 14:25 - 14:40 | Zoom                  |
| Break                                             |                       |
| Poster session C                  | 14:30 - 16:00 | gather.town room K-N  |
|---------------------------------------------------|                       |
| Break                                             |                       |
|---------------------------------------------------|                       |
| Keynote speaker 2 -- Roger Levy   | 19:00 - 19:45 | Zoom                  |
| Break                                             |                       |
| Poster session A                  | 20:00 - 21:30 | gather.town room K-N  |
| Oral session 3                    | 21:30 - 22:45 | Zoom                  |
| Break                                             |                       |
| Keynote speaker 3 -- Idan Blank   | 23:00 - 23:45 | Zoom                  |


Oral presentation session 1 (Time indications: Punta Cana)
---------------------------
- 5:15 - 5:27 _The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?_. Jasmijn Bastings and Katja Filippova .
- 5:27 - 5:39 _BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance_. R. Thomas Mccoy, Junghyun Min and Tal Linzen.
- 5:39 - 5:51 _Evaluating Attribution Methods using White-Box LSTMs_. Yiding Hao.
- 5:51 - 6:00 _Live Q&A_ with all paper authors.

Oral presentation session 2 (Time indications: Punta Cana)
---------------------------

- 12:00 - 12:12 _What Happens To BERT Embeddings During Fine-tuning?_. Amil Merchant, Elahe Rahimtoroghi, Ellie Pavlick and Ian Tenney.
- 12:12 - 12:24 _The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?_. Jasmijn Bastings and Katja Filippova  (repetition).
- 12:24 - 12:36 _Dissecting Lottery Ticket Transformers: Structural and Behavioral Study of Sparse Neural Machine Translation_. Rajiv Movva and Jason Zhao.
- 12:36 - 12:48 _The EOS Decision and Length Extrapolation_. Benjamin Newman, John Hewitt, Percy Liang and Christopher D. Manning.
- 12:48 - 13:00 _Live Q&A_ With all paper authors.

Oral presentation session 3 (Time indications: Punta Cana)
---------------------------
- 21:30 - 21:42 _Evaluating Attribution Methods using White-Box LSTMs_. Yiding Hao (repetition).
- 21:42 - 21:54 _The EOS Decision and Length Extrapolation_. Benjamin Newman, John Hewitt, Percy Liang and Christopher D. Manning (repetition).
- 21:54 - 22:06 _BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance_. R. Thomas Mccoy, Junghyun Min and Tal Linzen (repetition).
- 22:06 - 22:18 _Dissecting Lottery Ticket Transformers: Structural and Behavioral Study of Sparse Neural Machine Translation_. Rajiv Movva and Jason Zhao (repetition).
- 22:18 - 22:30  _What Happens To BERT Embeddings During Fine-tuning?_. Amil Merchant, Elahe Rahimtoroghi, Ellie Pavlick and Ian Tenney (repetition).
- 22:30 - 22:45 _Live Q&A_ With all paper authors.

Poster session 1 (block B)
----------------
**Archival papers**
- _BERTering RAMS: What and How Much does BERT Already Know About Event Arguments? - A Study on the RAMS Dataset_  Varun Gangal and Eduard Hovy.
- _Exploring Neural Entity Representations for Semantic Information_  Andrew Runge and Eduard Hovy.
- _Structured Self-Attention Weights Encodes Semantics in Sentiment Analysis_  Zhengxuan Wu, Thanh-Son Nguyen and Desmond Ong.
- _This is a BERT. Now there are several of them. Can they generalize to novel words?_  Coleman Haley.
- _Attacking Semantic Similarity: Generating Second-Order NLP Adversarial Examples_  John Morris.
- _Discovering the Compositional Structure of Vector Representations with Role Learning Networks_  Paul Soulos, R. Thomas Mccoy, Tal Linzen and Paul Smolensky.
- _Examining the rhetorical capacities of neural language models_  Zining Zhu, Chuer Pan, Mohamed Abdalla and Frank Rudzicz.
- _Linguistically-Informed Transformations (LIT): A Method for Automatically Generating Contrast Sets_  Chuanrong Li, Lin Shengshuo, Zeyu Liu, Xinyi Wu, Xuhui Zhou and Shane Steinert-Threlkeld.
- _Unsupervised Distillation of Syntactic Information from Contextualized Word Representations_  Shauli Ravfogel, Yanai Elazar, Jacob Goldberger and Yoav Goldberg.
- _Investigating Novel Verb Learning in BERT: Selectional Preference Classes and Alternation-Based Syntactic Generalization_  Tristan Thrush, Ethan Wilcox and Roger Levy.
- _Neural Natural Language Inference Models Partially Embed Theories of Lexical Entailment and Negation_  Atticus Geiger, Kyle Richardson and Christopher Potts.
- _Probing for Multilingual Numerical Understanding in Transformer-Based Language Models_  Devin Johnson, Denise Mak, Andrew Barker and Lexi Loessberg-Zahl. 
- _Do Language Embeddings capture Scales?_  Xikun Zhang, Deepak Ramachandran, Ian Tenney, Yanai Elazar and Dan Roth.
- _Searching for a Search Method: Benchmarking Search Algorithms for Generating NLP Adversarial Examples_  Jin Yong Yoo, John Morris, Eli Lifland and Yanjun Qi.

**Extended abstracts**
- _Inductive Biases of Seq2seq Learners under Minimal Stimuli_. Eugene Kharitonov and Rahma Chaabouni.
- _How Much Does RoBERTa Know About Quantifiers? An Assessment via Natural Language Inference_. Authors:  Cedegao Zhang.
- _Interpreting Neural Networks with Topic Models: Evidence from "Predicting In-game Actions From Interviews of NBA Players"_. Amir Feder, Nadav Oved and Roi Reichart.
- _BERT's Adaptability to Small Data_. Héctor Vázquez Martínez and Annika Heuser.
- _The Linear Geometry of Contextualized Word Representations_. Evan Hernandez and Jacob Andreas.
- _CausaLM: Causal Model Explanation Through Counterfactual Language Models_. Amir Feder, Nadav Oved, Uri Shalit and Roi Reichart.
- _Analyzing saliency in neural content scoring models for science explanations_. Brian Riordan, Sarah Bichler, Allison Bradford and Marcia Linn.
- _Mighty Morpho-Probing Models_. Naomi Tachikawa Shapiro, Amandalynne Paullada and Shane Steinert-Threlkeld.

**Findings papers**
- _Improving Text Understanding via Deep Syntax-Semantics Communication_. Hao Fei, Yafeng Ren and Donghong Ji.
- _Exploring BERT's sensitivity to lexical cues using tests from semantic priming_. Kanishka Misra, Allyson Ettinger and Julia Rayz. 
- _Corpora Evaluation and System Bias Detection in Multi-document Summarization_. Alvin Dey, Tanya Chowdhury, Yash Kumar and Tanmoy Chakraborty.
- _On the Language Neutrality of Pre-trained Multilingual Representations_. Jindřich Libovický, Rudolf Rosa and Alexander Fraser.
- _How Can Self-Attention Networks Recognize Dyck-n Languages?_. Javid Ebrahimi, Dhruv Gelda and Wei Zhang.
- _Why and when should you pool? Analyzing Pooling in Recurrent Architectures_. Pratyush Maini, Keshav Kolluru, Danish Pruthi and Mausam.
- _Rethinking Self-Attention: Towards Interpretability in Neural Parsing_. Khalil Mrini, Franck Dernoncourt, Quan Hung Tran, Trung Bui, Walter Chang and Ndapa Nakashole.
- _Reducing Sentiment Bias in Language Models via Counterfactual Evaluation_. Po-Sen Huang, Huan Zhang, Ray Jiang, Robert Stanforth, Johannes Welbl, Jack Rae, Vishal Maini, Dani Yogatama, Pushmeet Kohli.
- _Towards Controllable Biases in Language Generation_. Emily Sheng, Kai-Wei Chang, Prem Natarajan and Nanyun Peng.
- _Leakage-Adjusted Simulatability: Can Models Generate Non-Trivial Explanations of Their Behavior in Natural Language?_. Peter Hase, Shiyue Zhang, Harry Xie and Mohit Bansal.
- _Event-Related Bias Removal for Real-time Disaster Events_. Salvador Medina Maza, Evangelia Spiliopoulou, Eduard Hovy and Alexander Hauptmann. 
- _Undersensitivity in Neural Reading Comprehension_. Johannes Welbl, Pasquale Minervini, Max Bartolo, Pontus Stenetorp and Sebastian Riedel.
- _Natural Language Rationales with Full-Stack Visual Reasoning: From Pixels to Semantic Frames to Commonsense Graphs_. Ana Marasović, Chandra Bhagavatula, Jae sung Park, Ronan Le Bras, Noah A. Smith and Yejin Choi. 
- _Optimizing Word Segmentation for Downstream Task_. Tatsuya Hiraoka, Sho Takase, Kei Uchiumi, Atsushi Keyaki and Naoaki Okazaki.
- _Assessing Robustness of Text Classification through Maximal Safe Radius Computation_. Emanuele La Malfa, Min Wu, Luca Laurenti, Benjie Wang, Anthony Hartshorn and Marta Kwiatkowska. 
- _Evaluating Factuality in Generation with Dependency-level Entailment_. Tanya Goyal and Greg Durrett. 
- _Weakly- and Semi-supervised Evidence Extraction_. Danish Pruthi, Bhuwan Dhingra, Graham Neubig and Zachary C. Lipton.
- _On the Sub-Layer Functionalities of Transformer Decoder_ Yilin Yang, Longyue Wang, Shuming Shi, Prasad Tadepalli, Stefan Lee and Zhaopeng Tu.



Poster session 2 (block C)
----------------
**Archival papers**
- _BERTnesia: Investigating the capture and forgetting of knowledge in BERT_  Jaspreet Singh, Jonas Wallat and Avishek Anand.
- _Probing for Multilingual Numerical Understanding in Transformer-Based Language Models_  Devin Johnson, Denise Mak, Andrew Barker and Lexi Loessberg-Zahl.
- _Examining the rhetorical capacities of neural language models_  Zining Zhu, Chuer Pan, Mohamed Abdalla and Frank Rudzicz.
- _On the Interplay Between Fine-tuning and Sentence-Level Probing for Linguistic Knowledge in Pre-Trained Transformers_  Marius Mosbach, Anna Khokhlova, Michael A. Hedderich and Dietrich Klakow.
- _Emergent Language Generalization and Acquisition Speed are not tied to Compositionality_  Eugene Kharitonov and Marco Baroni.
- _Unsupervised Distillation of Syntactic Information from Contextualized Word Representations_  Shauli Ravfogel, Yanai Elazar, Jacob Goldberger and Yoav Goldberg.
- _It’s not Greek to mBERT: Inducing Word-Level Translations from Multilingual BERT_  Hila Gonen, Shauli Ravfogel, Yanai Elazar and Yoav Goldberg.
- _Investigating Novel Verb Learning in BERT: Selectional Preference Classes and Alternation-Based Syntactic Generalization_  Tristan Thrush, Ethan Wilcox and Roger Levy.
- _Discovering the Compositional Structure of Vector Representations with Role Learning Networks_  Paul Soulos, R. Thomas Mccoy, Tal Linzen and Paul Smolensky.
- _Unsupervised Evaluation for Question Answering with Transformers_  Lukas Muttenthaler, Isabelle Augenstein and Johannes Bjerva.
- _This is a BERT. Now there are several of them. Can they generalize to novel words?_  Coleman Haley.
- _Leveraging Extracted Model Adversaries for Improved Black Box Attacks_  Naveen Jafer Nizar and Ari Kobren.
- _Latent Tree Learning with Ordered Neurons: What Parses Does It Produce?_  Yian Zhang.
- _BERTering RAMS: What and How Much does BERT Already Know About Event Arguments? - A Study on the RAMS Dataset_  Varun Gangal and Eduard Hovy.
- _The Explanation Game: Towards Prediction Explainability through Sparse Communication_  Marcos Treviso and André F. T. Martins.
- _Controlling the Imprint of Passivization and Negation in Contextualized Representations_  Hande Celikkanat, Sami Virpioja, Jörg Tiedemann and Marianna Apidianaki.
- _How does BERT capture semantics? A closer look at polysemous words_  David Yenicelik, Florian Schmidt and Yannic Kilcher.
- _Exploring Neural Entity Representations for Semantic Information_  Andrew Runge and Eduard Hovy.
- _Structured Self-Attention Weights Encodes Semantics in Sentiment Analysis_  Zhengxuan Wu, Thanh-Son Nguyen and Desmond Ong.
- _Defining Explanation in an AI Context_  Tejaswani Verma, Christoph Lingenfelder and Dietrich Klakow.

**Extended abstracts**
- _Analyzing Neural Machine Translation Trained Using Targeted Part-Of-Speech_. Subhadarshi Panda.
- _BERT's Adaptability to Small Data_. Héctor Vázquez Martínez and Annika Heuser.

**Findings papers**
- _Optimizing Word Segmentation for Downstream Task_. Tatsuya Hiraoka, Sho Takase, Kei Uchiumi, Atsushi Keyaki and Naoaki Okazaki.
- _Universal Dependencies According to BERT: Both More Specific andMore General_. Tomasz Limisiewicz, David Mareček and Rudolf Rosa.
- _Natural Language Rationales with Full-Stack Visual Reasoning: From Pixels to Semantic Frames to Commonsense Graphs_. Ana Marasović, Chandra Bhagavatula, Jae sung Park, Ronan Le Bras, Noah A. Smith and Yejin Choi.
- _Evaluating Factuality in Generation with Dependency-level Entailment_. Tanya Goyal and Greg Durrett.
- _What's so special about BERT's layers? A closer look at the NLP pipeline in monolingual and multilingual models_. Wietse de Vries, Andreas van Cranenburgh and Malvina Nissim.
- _On the Sub-Layer Functionalities of Transformer Decoder_ Yilin Yang, Longyue Wang, Shuming Shi, Prasad Tadepalli, Stefan Lee and Zhaopeng Tu.
- _Leakage-Adjusted Simulatability: Can Models Generate Non-Trivial Explanations of Their Behavior in Natural Language?_. Peter Hase, Shiyue Zhang, Harry Xie and Mohit Bansal.
- _Interpretable Entity Representations through Large-Scale Typing_. Yasumasa Onoe and Greg Durrett.
- _Towards Controllable Biases in Language Generation_. Emily Sheng, Kai-Wei Chang, Prem Natarajan and Nanyun Peng.
- _Be Different to Be Better! A Benchmark to Leverage the Complementarity of Language and Vision_. Sandro Pezzelle, Claudio Greco, Greta Gandolfi, Eleonora Gualdoni and Raffaella Bernardi.
- _How Can Self-Attention Networks Recognize Dyck-n Languages?_. Javid Ebrahimi, Dhruv Gelda and Wei Zhang.
- _NLP Service APIs and Models for Efficient Registration of New Clients_. Sahil Shah, Vihari Piratla, Soumen Chakrabarti and Sunita Sarawagi. 
- _Why and when should you pool? Analyzing Pooling in Recurrent Architectures_. Pratyush Maini, Keshav Kolluru, Danish Pruthi and Mausam. 
- _Exploring BERT's sensitivity to lexical cues using tests from semantic priming_. Kanishka Misra, Allyson Ettinger and Julia Rayz. 
- _LSTMS Compose---and Learn---Bottom-Up_. Naomi Saphra and Adam Lopez. 

Poster session 3 (block A)
----------------
**Archival papers**
- _The Explanation Game: Towards Prediction Explainability through Sparse Communication_  Marcos Treviso and André F. T. Martins.
- _Defining Explanation in an AI Context_  Tejaswani Verma, Christoph Lingenfelder and Dietrich Klakow .
- _Controlling the Imprint of Passivization and Negation in Contextualized Representations_  Hande Celikkanat, Sami Virpioja, Jörg Tiedemann and Marianna Apidianaki .
- _How does BERT capture semantics? A closer look at polysemous words_  David Yenicelik, Florian Schmidt and Yannic Kilcher .
- _Leveraging Extracted Model Adversaries for Improved Black Box Attacks_. Naveen Jafer Nizar and Ari Kobren.
- _Attacking Semantic Similarity: Generating Second-Order NLP Adversarial Examples_  John Morris.
- _BERTnesia: Investigating the capture and forgetting of knowledge in BERT_  Jaspreet Singh, Jonas Wallat and Avishek Anand.
- _Linguistically-Informed Transformations (LIT): A Method for Automatically Generating Contrast Sets_  Chuanrong Li, Lin Shengshuo, Zeyu Liu, Xinyi Wu, Xuhui Zhou and Shane Steinert-Threlkeld.
- _Latent Tree Learning with Ordered Neurons: What Parses Does It Produce?_  Yian Zhang.
- _Emergent Language Generalization and Acquisition Speed are not tied to Compositionality_. Eugene Kharitonov and Marco Baroni.
- _Neural Natural Language Inference Models Partially Embed Theories of Lexical Entailment and Negation_  Atticus Geiger, Kyle Richardson and Christopher Potts.
- _It’s not Greek to mBERT: Inducing Word-Level Translations from Multilingual BERT_. Hila Gonen, Shauli Ravfogel, Yanai Elazar and Yoav Goldberg.
- _On the Interplay Between Fine-tuning and Sentence-Level Probing for Linguistic Knowledge in Pre-Trained Transformers_. Marius Mosbach, Anna Khokhlova, Michael A. Hedderich and Dietrich Klakow.
- _Unsupervised Evaluation for Question Answering with Transformers_. Lukas Muttenthaler, Isabelle Augenstein and Johannes Bjerva.
- _Do Language Embeddings capture Scales?_  Xikun Zhang, Deepak Ramachandran, Ian Tenney, Yanai Elazar and Dan Roth.
- _Searching for a Search Method: Benchmarking Search Algorithms for Generating NLP Adversarial Examples_  Jin Yong Yoo, John Morris, Eli Lifland and Yanjun Qi.

**Extended abstracts**
- _How Much Does RoBERTa Know About Quantifiers? An Assessment via Natural Language Inference_. Authors:  Cedegao Zhang.
- _Inductive Biases of Seq2seq Learners under Minimal Stimuli_. Eugene Kharitonov and Rahma Chaabouni.
- _Interpreting Neural Networks with Topic Models: Evidence from "Predicting In-game Actions From Interviews of NBA Players"_. Amir Feder, Nadav Oved and Roi Reichart.
- _The Linear Geometry of Contextualized Word Representations_. Evan Hernandez and Jacob Andreas.
- _Mighty Morpho-Probing Models_. Naomi Tachikawa Shapiro, Amandalynne Paullada and Shane Steinert-Threlkeld.
- _CausaLM: Causal Model Explanation Through Counterfactual Language Models_. Amir Feder, Nadav Oved, Uri Shalit and Roi Reichart.
- _Analyzing saliency in neural content scoring models for science explanations_. Brian Riordan, Sarah Bichler, Allison Bradford and Marcia Linn. 
- _Analyzing Neural Machine Translation Trained Using Targeted Part-Of-Speech_. Subhadarshi Panda.

**Findings papers**
- _Improving Text Understanding via Deep Syntax-Semantics Communication_. Hao Fei, Yafeng Ren and Donghong Ji.
- _On the Language Neutrality of Pre-trained Multilingual Representations_. Jindřich Libovický, Rudolf Rosa and Alexander Fraser.
- _Corpora Evaluation and System Bias Detection in Multi-document Summarization_. Alvin Dey, Tanya Chowdhury, Yash Kumar and Tanmoy Chakraborty.
- _Interpretable Entity Representations through Large-Scale Typing_. Yasumasa Onoe and Greg Durrett.
- _Be Different to Be Better! A Benchmark to Leverage the Complementarity of Language and Vision_. Sandro Pezzelle, Claudio Greco, Greta Gandolfi, Eleonora Gualdoni and Raffaella Bernardi.
- _NLP Service APIs and Models for Efficient Registration of New Clients_. Sahil Shah, Vihari Piratla, Soumen Chakrabarti and Sunita Sarawagi.
- _Rethinking Self-Attention: Towards Interpretability in Neural Parsing_. Khalil Mrini, Franck Dernoncourt, Quan Hung Tran, Trung Bui, Walter Chang and Ndapa Nakashole.
- _Reducing Sentiment Bias in Language Models via Counterfactual Evaluation_. Po-Sen Huang, Huan Zhang, Ray Jiang, Robert Stanforth, Johannes Welbl, Jack Rae, Vishal Maini, Dani Yogatama, Pushmeet Kohli.
- _Undersensitivity in Neural Reading Comprehension_. Johannes Welbl, Pasquale Minervini, Max Bartolo, Pontus Stenetorp and Sebastian Riedel.
- _Assessing Robustness of Text Classification through Maximal Safe Radius Computation_. Emanuele La Malfa, Min Wu, Luca Laurenti, Benjie Wang, Anthony Hartshorn and Marta Kwiatkowska. 
- _Universal Dependencies According to BERT: Both More Specific andMore General_. Tomasz Limisiewicz, David Mareček and Rudolf Rosa.
- _LSTMS Compose---and Learn---Bottom-Up_. Naomi Saphra and Adam Lopez.
- _What's so special about BERT's layers? A closer look at the NLP pipeline in monolingual and multilingual models_. Wietse de Vries, Andreas van Cranenburgh and Malvina Nissim.
